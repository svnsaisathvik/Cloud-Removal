{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6654e212",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1 â€” Imports + small utilities\n",
    "import os\n",
    "import glob\n",
    "import json\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import rasterio\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "\n",
    "from terratorch import BACKBONE_REGISTRY\n",
    "import torch.nn as nn\n",
    "\n",
    "# For plotting (visualization cell)\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8957ca85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train path: D:\\TerraMind_Dataset\\train\n",
      "Val   path: D:\\TerraMind_Dataset\\val\n",
      "Output dir: c:\\Users\\sathv\\OneDrive\\Desktop\\Sathvik education\\5th Semester\\PE\\terramind_training_output\n"
     ]
    }
   ],
   "source": [
    "# Cell 2 â€” Paths (change these to your paths if different)\n",
    "TRAIN_DATA_PATH = r\"D:\\TerraMind_Dataset\\train\"\n",
    "VAL_DATA_PATH   = r\"D:\\TerraMind_Dataset\\val\"\n",
    "\n",
    "# Where to store the mapping and outputs\n",
    "LABEL_MAP_FILE = \"label_mapping.json\"\n",
    "OUTPUT_DIR = Path(\"./terramind_training_output\")\n",
    "OUTPUT_DIR.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "print(\"Train path:\", TRAIN_DATA_PATH)\n",
    "print(\"Val   path:\", VAL_DATA_PATH)\n",
    "print(\"Output dir:\", OUTPUT_DIR.absolute())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "97349ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install --force-reinstall \"numpy<2.0\" numpy==1.26.4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e1bb34d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install --force-reinstall pytorch-lightning lightning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a86deffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install --force-reinstall \"numpy==1.26.4\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "66a8fadf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading existing mapping from label_mapping.json\n",
      "Total unique raw labels found: 5888\n",
      "Example raw labels (first 20): [978, 985, 986, 988, 989, 990, 991, 992, 993, 994, 995, 996, 997, 998, 999, 1000, 1001, 1002, 1003, 1004]\n"
     ]
    }
   ],
   "source": [
    "# Cell 3 â€” Build or load label mapping (remaps raw mask values -> 0..N-1)\n",
    "def build_label_mapping_from_dirs(mask_dirs):\n",
    "    \"\"\"Scan provided mask directories and return sorted unique values -> mapping dict.\"\"\"\n",
    "    all_vals = set()\n",
    "    for d in mask_dirs:\n",
    "        files = glob.glob(os.path.join(d, \"*.tif\")) + glob.glob(os.path.join(d, \"*.tiff\"))\n",
    "        for p in files:\n",
    "            try:\n",
    "                with rasterio.open(p) as src:\n",
    "                    m = src.read(1)\n",
    "                    all_vals.update(np.unique(m).tolist())\n",
    "            except Exception as e:\n",
    "                # keep scanning even if a file is broken\n",
    "                print(f\"Warning reading {p}: {e}\")\n",
    "    all_vals = sorted([int(v) for v in all_vals])\n",
    "    label_map = {raw: i for i, raw in enumerate(all_vals)}\n",
    "    return label_map, all_vals\n",
    "\n",
    "# If existing mapping file present, load it; otherwise build from train masks (and val)\n",
    "if os.path.exists(LABEL_MAP_FILE):\n",
    "    print(f\"Loading existing mapping from {LABEL_MAP_FILE}\")\n",
    "    with open(LABEL_MAP_FILE, \"r\") as f:\n",
    "        label_map = json.load(f)\n",
    "    # keys in JSON were strings â€” ensure they are ints as keys\n",
    "    label_map = {int(k): int(v) for k, v in label_map.items()}\n",
    "    unique_labels = sorted(label_map.keys())\n",
    "else:\n",
    "    print(\"Building label mapping from train+val masks (this may take a while)...\")\n",
    "    mask_dirs = [os.path.join(TRAIN_DATA_PATH, \"masks\"), os.path.join(VAL_DATA_PATH, \"masks\")]\n",
    "    label_map, unique_labels = build_label_mapping_from_dirs(mask_dirs)\n",
    "    with open(LABEL_MAP_FILE, \"w\") as f:\n",
    "        json.dump({int(k): int(v) for k, v in label_map.items()}, f)\n",
    "    print(f\"Saved {LABEL_MAP_FILE}\")\n",
    "\n",
    "print(\"Total unique raw labels found:\", len(unique_labels))\n",
    "print(\"Example raw labels (first 20):\", unique_labels[:20])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4b2b6de7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONFIG:\n",
      "  image_size: 224\n",
      "  batch_size: 1\n",
      "  num_classes: 5888\n",
      "  learning_rate: 0.0001\n",
      "  max_epochs: 1\n",
      "  precision: 32\n",
      "  model_name: terramind_v1_base\n",
      "  modalities: ['S2L2A']\n",
      "  bands: None\n"
     ]
    }
   ],
   "source": [
    "# Cell 4 â€” Configuration (num_classes set from mapping)\n",
    "CONFIG = {\n",
    "    \"image_size\": 224,\n",
    "    \"batch_size\": 1,\n",
    "    \"num_classes\": len(label_map),   # ðŸ”§ FIX: set from mapping\n",
    "    \"learning_rate\": 1e-4,\n",
    "    \"max_epochs\": 1,\n",
    "    \"precision\": 32,\n",
    "    \"model_name\": \"terramind_v1_base\",\n",
    "    \"modalities\": [\"S2L2A\"],\n",
    "    \"bands\": None\n",
    "}\n",
    "\n",
    "print(\"CONFIG:\")\n",
    "for k,v in CONFIG.items():\n",
    "    print(f\"  {k}: {v}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "85d92de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5 â€” Dataset with applied mapping\n",
    "class TerraMindDataset(Dataset):\n",
    "    def __init__(self, data_path, image_size=224, label_map=None):\n",
    "        self.data_path = Path(data_path)\n",
    "        self.images_path = self.data_path / \"images\"\n",
    "        self.masks_path  = self.data_path / \"masks\"\n",
    "        self.image_size = image_size\n",
    "        self.label_map = label_map or {}\n",
    "\n",
    "        self.samples = sorted([p.stem for p in self.images_path.glob(\"*.tif\")] + \n",
    "                              [p.stem for p in self.images_path.glob(\"*.tiff\")])\n",
    "\n",
    "        print(f\"âœ“ Found {len(self.samples)} samples in {data_path}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def _remap_mask(self, mask_np):\n",
    "        \"\"\"\n",
    "        ðŸ”§ FIX: remap raw mask values -> contiguous integer labels 0..N-1\n",
    "        (Changed: CrossEntropyLoss requires contiguous class IDs)\n",
    "        \"\"\"\n",
    "        # Use vectorized lookup through numpy; ensure dtype is integer\n",
    "        # label_map.get ensures unmapped values return -1 (we catch that)\n",
    "        get_fn = np.vectorize(lambda x: self.label_map.get(int(x), -1))\n",
    "        mapped = get_fn(mask_np).astype(np.int64)\n",
    "        if (mapped < 0).any():\n",
    "            # find what values were unmapped (diagnostic)\n",
    "            unmapped = np.unique(mask_np[mapped < 0])\n",
    "            raise ValueError(f\"Found unmapped mask values: {unmapped[:10]} (first 10). Update label_map.\")\n",
    "        return mapped\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample_id = self.samples[idx]\n",
    "\n",
    "        # ---------------- IMAGE ----------------\n",
    "        img_path = self.images_path / f\"{sample_id}.tif\"\n",
    "        if not img_path.exists():\n",
    "            img_path = self.images_path / f\"{sample_id}.tiff\"\n",
    "\n",
    "        with rasterio.open(str(img_path)) as src:\n",
    "            img = src.read()\n",
    "\n",
    "        # FORCE 12 BANDS FOR TERRAMIND\n",
    "        if img.shape[0] > 12:\n",
    "            img = img[:12]\n",
    "        if img.shape[0] < 12:\n",
    "            raise ValueError(\"Image has less than 12 bands. TerraMind S2L2A requires 12.\")\n",
    "\n",
    "        img = img.astype(np.float32)\n",
    "        if img.max() > 2.0:\n",
    "            img = img / 10000.0\n",
    "        img = np.clip(img, 0, 1)\n",
    "\n",
    "        img_tensor = torch.from_numpy(img).unsqueeze(0)  # shape (1, bands, H, W)\n",
    "        img_tensor = F.interpolate(\n",
    "            img_tensor,\n",
    "            size=(self.image_size, self.image_size),\n",
    "            mode=\"bilinear\",\n",
    "            align_corners=False\n",
    "        ).squeeze(0)  # back to (bands, H, W)\n",
    "\n",
    "        data = {\"S2L2A\": img_tensor}\n",
    "\n",
    "        # ---------------- MASK ----------------\n",
    "        mask_name = sample_id.replace(\"img_\", \"mask_\")\n",
    "        mask_path = self.masks_path / f\"{mask_name}.tif\"\n",
    "        if not mask_path.exists():\n",
    "            mask_path = self.masks_path / f\"{mask_name}.tiff\"\n",
    "\n",
    "        with rasterio.open(str(mask_path)) as src:\n",
    "            mask = src.read(1)\n",
    "\n",
    "        # ðŸ”§ FIX: remap raw labels -> contiguous indices BEFORE interpolation\n",
    "        mask_mapped = self._remap_mask(mask)\n",
    "\n",
    "        # Convert to tensor, interpolate with nearest neighbor, then long\n",
    "        mask_t = torch.from_numpy(mask_mapped).unsqueeze(0).unsqueeze(0).float()\n",
    "        mask_t = F.interpolate(mask_t, size=(self.image_size, self.image_size), mode=\"nearest\")\n",
    "        label = mask_t.squeeze(0).squeeze(0).long()  # final shape (H, W), dtype long\n",
    "\n",
    "        return data, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "32340360",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Found 228 samples in D:\\TerraMind_Dataset\\train\n",
      "âœ“ Found 57 samples in D:\\TerraMind_Dataset\\val\n",
      "DataLoaders ready!\n",
      "Train samples: 228\n",
      "Val samples: 57\n",
      "Data keys: ['S2L2A']\n",
      "  S2L2A: torch.Size([1, 12, 224, 224]), range=[0.001, 0.388]\n",
      "Label shape: torch.Size([1, 224, 224])\n",
      "Label min/max/unique-sample: 3178 5277 tensor([3178, 3200, 3203, 3233, 3246, 3265, 3267, 3272, 3280, 3286, 3291, 3295,\n",
      "        3298, 3303, 3305, 3307, 3315, 3319, 3323, 3324])\n",
      "CONFIG num_classes: 5888\n"
     ]
    }
   ],
   "source": [
    "# Cell 6 â€” Create datasets & dataloaders (pass label_map)\n",
    "# ðŸ”§ FIX: pass label_map into datasets so masks are remapped.\n",
    "train_dataset = TerraMindDataset(TRAIN_DATA_PATH, image_size=CONFIG[\"image_size\"], label_map=label_map)\n",
    "val_dataset   = TerraMindDataset(VAL_DATA_PATH,   image_size=CONFIG[\"image_size\"], label_map=label_map)\n",
    "\n",
    "# Windows + rasterio: use num_workers=0\n",
    "train_loader = DataLoader(train_dataset, batch_size=CONFIG[\"batch_size\"], shuffle=True,\n",
    "                          num_workers=0, pin_memory=False, drop_last=True)\n",
    "val_loader   = DataLoader(val_dataset,   batch_size=CONFIG[\"batch_size\"], shuffle=False,\n",
    "                          num_workers=0, pin_memory=False)\n",
    "\n",
    "print(\"DataLoaders ready!\")\n",
    "print(\"Train samples:\", len(train_dataset))\n",
    "print(\"Val samples:\", len(val_dataset))\n",
    "\n",
    "# Quick sanity check: sample a batch and inspect mapped labels\n",
    "batch = next(iter(train_loader))\n",
    "data_sample, labels_sample = batch\n",
    "print(\"Data keys:\", list(data_sample.keys()))\n",
    "for k, v in data_sample.items():\n",
    "    print(f\"  {k}: {v.shape}, range=[{v.min():.3f}, {v.max():.3f}]\")\n",
    "print(\"Label shape:\", labels_sample.shape)\n",
    "print(\"Label min/max/unique-sample:\", labels_sample.min().item(), labels_sample.max().item(), torch.unique(labels_sample)[:20])\n",
    "print(\"CONFIG num_classes:\", CONFIG[\"num_classes\"])\n",
    "assert labels_sample.min().item() >= 0 and labels_sample.max().item() < CONFIG[\"num_classes\"], \\\n",
    "    \"Mapped labels not in expected range!\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f66e00",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Cell 7 â€” Model definition (same as yours, with small comment)\n",
    "class TerraMindFineTuner(pl.LightningModule):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.backbone = BACKBONE_REGISTRY.build(\n",
    "            config[\"model_name\"],\n",
    "            pretrained=True,\n",
    "            modalities=config[\"modalities\"],\n",
    "            bands=config[\"bands\"],\n",
    "            merge_method=\"mean\"\n",
    "        )\n",
    "\n",
    "        # ðŸ”§ NOTE: decoder output channels must equal CONFIG[\"num_classes\"]\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Conv2d(768, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, config[\"num_classes\"], kernel_size=1)\n",
    "        )\n",
    "\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, data):\n",
    "        out = self.backbone(data)\n",
    "\n",
    "        if isinstance(out, list):\n",
    "            tokens = out[0]\n",
    "        else:\n",
    "            tokens = out\n",
    "\n",
    "        B, N, C = tokens.shape\n",
    "        H = W = int(N ** 0.5)\n",
    "\n",
    "        feat = tokens.transpose(1, 2).reshape(B, C, H, W)\n",
    "        feat = F.interpolate(feat, size=(self.config[\"image_size\"], self.config[\"image_size\"]), mode=\"bilinear\")\n",
    "\n",
    "        return self.decoder(feat)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        data, y = batch\n",
    "        y_hat = self.forward(data)\n",
    "\n",
    "        # ðŸ”§ Extra safety: check target range (will raise early with informative message)\n",
    "        if y.max() >= self.config[\"num_classes\"] or y.min() < 0:\n",
    "            raise ValueError(f\"Label out of range: min={y.min().item()}, max={y.max().item()} for num_classes={self.config['num_classes']}\")\n",
    "\n",
    "        loss = self.loss_fn(y_hat, y)\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        data, y = batch\n",
    "        y_hat = self.forward(data)\n",
    "        loss = self.loss_fn(y_hat, y)\n",
    "        self.log(\"val_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.AdamW(self.parameters(), lr=self.config[\"learning_rate\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc6d5b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outputs will be saved to: c:\\Users\\sathv\\OneDrive\\Desktop\\Sathvik education\\5th Semester\\PE\\terramind_training_output\n",
      "\n",
      "Initializing model...\n"
     ]
    }
   ],
   "source": [
    "# Cell 8 â€” Trainer, callbacks, logger\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "\n",
    "output_dir = OUTPUT_DIR\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    dirpath=output_dir / 'checkpoints',\n",
    "    filename='terramind-{epoch:02d}-{val_loss:.4f}',\n",
    "    save_top_k=3,\n",
    "    monitor='val_loss',\n",
    "    mode='min',\n",
    "    save_last=True,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "early_stop_callback = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=8,\n",
    "    mode='min',\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "logger = TensorBoardLogger(\n",
    "    save_dir=output_dir / 'logs',\n",
    "    name='terramind_finetuning'\n",
    ")\n",
    "\n",
    "print(f\"Outputs will be saved to: {output_dir.absolute()}\")\n",
    "\n",
    "print(\"\\nInitializing model...\")\n",
    "model = TerraMindFineTuner(CONFIG)\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Model params: total={total_params:,}, trainable={trainable_params:,}\")\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=CONFIG['max_epochs'],\n",
    "    accelerator='gpu' if torch.cuda.is_available() else 'cpu',\n",
    "    devices=1,\n",
    "    precision=CONFIG['precision'],\n",
    "    callbacks=[checkpoint_callback, early_stop_callback],\n",
    "    logger=logger,\n",
    "    log_every_n_steps=10,\n",
    "    gradient_clip_val=1.0,\n",
    "    accumulate_grad_batches=2,\n",
    "    enable_progress_bar=True,\n",
    "    enable_model_summary=True\n",
    ")\n",
    "print(\"Trainer initialized.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9254876c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "ðŸš€ STARTING TRAINING\n",
      "============================================================\n",
      "Estimated time: ~3 minutes\n",
      "============================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name     | Type             | Params | Mode \n",
      "------------------------------------------------------\n",
      "0 | backbone | TerraMindViT     | 87.3 M | train\n",
      "1 | decoder  | Sequential       | 2.8 M  | train\n",
      "2 | loss_fn  | CrossEntropyLoss | 0      | train\n",
      "------------------------------------------------------\n",
      "90.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "90.1 M    Total params\n",
      "360.553   Total estimated model params size (MB)\n",
      "182       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23a0697765e14a0f9a605366ac03b6f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sathv\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:433: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\sathv\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:433: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13c2e1097f404402aa48e3056a219f77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: \n",
      "Detected KeyboardInterrupt, attempting graceful shutdown ...\n",
      "INFO:lightning.pytorch.utilities.rank_zero:\n",
      "Detected KeyboardInterrupt, attempting graceful shutdown ...\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "1",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[31mSystemExit\u001b[39m\u001b[31m:\u001b[39m 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sathv\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:3707: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "# Cell 9 â€” Training loop (with try/except for diagnostics)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ðŸš€ STARTING TRAINING\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Estimated time: ~{len(train_loader) * CONFIG['max_epochs'] // 60} minutes\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "try:\n",
    "    trainer.fit(\n",
    "        model,\n",
    "        train_dataloaders=train_loader,\n",
    "        val_dataloaders=val_loader\n",
    "    )\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"âœ… TRAINING COMPLETED!\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"\\nBest checkpoint: {checkpoint_callback.best_model_path}\")\n",
    "    print(f\"Best val loss: {checkpoint_callback.best_model_score:.4f}\")\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\nâš ï¸ Training interrupted\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nâŒ Training error: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b88bbfa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONFIG['num_classes'] = 5888\n"
     ]
    }
   ],
   "source": [
    "# Cell 10 â€” Post-train: load best model and evaluate (optional)\n",
    "if checkpoint_callback.best_model_path and os.path.exists(checkpoint_callback.best_model_path):\n",
    "    print(\"\\nðŸ”„ Loading best model...\")\n",
    "    best_model = TerraMindFineTuner.load_from_checkpoint(checkpoint_callback.best_model_path, config=CONFIG)\n",
    "    best_model.eval()\n",
    "    if torch.cuda.is_available():\n",
    "        best_model = best_model.cuda()\n",
    "\n",
    "    print(\"Evaluating on validation dataset...\")\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader, desc=\"Evaluating\"):\n",
    "            data, labels = batch\n",
    "            if torch.cuda.is_available():\n",
    "                data = {k: v.cuda() for k, v in data.items()}\n",
    "                labels = labels.cuda()\n",
    "\n",
    "            outputs = best_model(data)\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "\n",
    "            all_preds.append(preds.cpu())\n",
    "            all_labels.append(labels.cpu())\n",
    "\n",
    "    all_preds = torch.cat(all_preds)\n",
    "    all_labels = torch.cat(all_labels)\n",
    "\n",
    "    accuracy = (all_preds == all_labels).float().mean()\n",
    "    print(f\"Validation accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
    "else:\n",
    "    print(\"No checkpoint found, skipping evaluation.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa983a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11 â€” Visualization utility (optional)\n",
    "def visualize_predictions(model, dataloader, num_samples=4, output_path=OUTPUT_DIR / \"predictions.png\"):\n",
    "    model.eval()\n",
    "    fig, axes = plt.subplots(num_samples, 3, figsize=(15, num_samples*4))\n",
    "    if num_samples == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (data, labels) in enumerate(dataloader):\n",
    "            if i >= num_samples:\n",
    "                break\n",
    "            if torch.cuda.is_available():\n",
    "                data = {k: v.cuda() for k, v in data.items()}\n",
    "                labels = labels.cuda()\n",
    "\n",
    "            outputs = model(data)\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "\n",
    "            first_modality = list(data.keys())[0]\n",
    "            img = data[first_modality][0].cpu().numpy()\n",
    "\n",
    "            # RGB visualization if available\n",
    "            if img.shape[0] >= 3:\n",
    "                rgb = img[:3].transpose(1, 2, 0)\n",
    "                rgb = np.clip((rgb - rgb.min()) / (rgb.max() - rgb.min() + 1e-8), 0, 1)\n",
    "            else:\n",
    "                rgb = img[0]\n",
    "                rgb = (rgb - rgb.min()) / (rgb.max() - rgb.min() + 1e-8)\n",
    "\n",
    "            axes[i, 0].imshow(rgb)\n",
    "            axes[i, 0].set_title(f'Input ({first_modality})')\n",
    "            axes[i, 0].axis('off')\n",
    "\n",
    "            axes[i, 1].imshow(labels[0].cpu().numpy(), cmap='tab20')\n",
    "            axes[i, 1].set_title('Ground Truth')\n",
    "            axes[i, 1].axis('off')\n",
    "\n",
    "            axes[i, 2].imshow(preds[0].cpu().numpy(), cmap='tab20')\n",
    "            axes[i, 2].set_title('Prediction')\n",
    "            axes[i, 2].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_path, dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(f\"Saved visualization to: {output_path}\")\n",
    "\n",
    "# Use it like:\n",
    "# visualize_predictions(best_model if 'best_model' in locals() else model, val_loader, num_samples=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7327118e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Found 228 samples in D:\\TerraMind_Dataset\\train\n",
      "âœ“ Found 57 samples in D:\\TerraMind_Dataset\\val\n",
      "DataLoaders ready!\n",
      "Train samples: 228\n",
      "Val samples: 57\n"
     ]
    }
   ],
   "source": [
    "# Cell 12 â€” Save final model + config\n",
    "final_dir = OUTPUT_DIR / 'final_model'\n",
    "final_dir.mkdir(exist_ok=True)\n",
    "if 'best_model' in globals():\n",
    "    torch.save(best_model.state_dict(), final_dir / 'weights.pth')\n",
    "else:\n",
    "    torch.save(model.state_dict(), final_dir / 'weights.pth')\n",
    "\n",
    "with open(final_dir / 'config.json', 'w') as f:\n",
    "    json.dump(CONFIG, f, indent=2)\n",
    "\n",
    "print(f\"Final model saved to: {final_dir.absolute()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14190266",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“ Exploring: D:\\TerraMind_Dataset\\train\n",
      "============================================================\n",
      "\n",
      "Found 2 subdirectories:\n",
      "  ðŸ“‚ images/\n",
      "     Files: 228\n",
      "     Examples: ['img_00000.tif', 'img_00001.tif', 'img_00002.tif']\n",
      "     Extensions: {'.tif'}\n",
      "  ðŸ“‚ masks/\n",
      "     Files: 228\n",
      "     Examples: ['mask_00000.tif', 'mask_00001.tif', 'mask_00002.tif']\n",
      "     Extensions: {'.tif'}\n",
      "\n",
      "ðŸ“ Exploring: D:\\TerraMind_Dataset\\val\n",
      "============================================================\n",
      "\n",
      "Found 2 subdirectories:\n",
      "  ðŸ“‚ images/\n",
      "     Files: 57\n",
      "     Examples: ['img_00000.tif', 'img_00001.tif', 'img_00002.tif']\n",
      "     Extensions: {'.tif'}\n",
      "  ðŸ“‚ masks/\n",
      "     Files: 57\n",
      "     Examples: ['mask_00000.tif', 'mask_00001.tif', 'mask_00002.tif']\n",
      "     Extensions: {'.tif'}\n"
     ]
    }
   ],
   "source": [
    "# def explore_dataset(data_path):\n",
    "#     print(f\"\\nðŸ“ Exploring: {data_path}\")\n",
    "#     print(\"=\"*60)\n",
    "    \n",
    "#     if not os.path.exists(data_path):\n",
    "#         print(f\"âŒ Path does not exist!\")\n",
    "#         return None\n",
    "    \n",
    "#     subdirs = [d for d in os.listdir(data_path) if os.path.isdir(os.path.join(data_path, d))]\n",
    "#     print(f\"\\nFound {len(subdirs)} subdirectories:\")\n",
    "#     for subdir in subdirs:\n",
    "#         subdir_path = os.path.join(data_path, subdir)\n",
    "#         files = [f for f in os.listdir(subdir_path) if os.path.isfile(os.path.join(subdir_path, f))]\n",
    "#         print(f\"  ðŸ“‚ {subdir}/\")\n",
    "#         print(f\"     Files: {len(files)}\")\n",
    "#         if files:\n",
    "#             print(f\"     Examples: {files[:3]}\")\n",
    "#             extensions = set([os.path.splitext(f)[1] for f in files])\n",
    "#             print(f\"     Extensions: {extensions}\")\n",
    "    \n",
    "#     return subdirs\n",
    "\n",
    "# # Explore your data\n",
    "# train_subdirs = explore_dataset(TRAIN_DATA_PATH)\n",
    "# val_subdirs = explore_dataset(VAL_DATA_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f867bcb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "import rasterio\n",
    "from pathlib import Path\n",
    "\n",
    "class TerraMindDataset(Dataset):\n",
    "    def __init__(self, data_path, image_size=224):\n",
    "        self.data_path = Path(data_path)\n",
    "        self.images_path = self.data_path / \"images\"\n",
    "        self.masks_path  = self.data_path / \"masks\"\n",
    "        self.image_size = image_size\n",
    "\n",
    "        self.samples = sorted([\n",
    "            p.stem for p in self.images_path.glob(\"*.tif\")\n",
    "        ] + [\n",
    "            p.stem for p in self.images_path.glob(\"*.tiff\")\n",
    "        ])\n",
    "\n",
    "        print(f\"âœ“ Found {len(self.samples)} samples in {data_path}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample_id = self.samples[idx]\n",
    "\n",
    "        # ---------------- IMAGE ----------------\n",
    "        img_path = self.images_path / f\"{sample_id}.tif\"\n",
    "        if not img_path.exists():\n",
    "            img_path = self.images_path / f\"{sample_id}.tiff\"\n",
    "\n",
    "        with rasterio.open(str(img_path)) as src:\n",
    "            img = src.read()\n",
    "\n",
    "        # FORCE 12 BANDS FOR TERRAMIND\n",
    "        if img.shape[0] > 12:\n",
    "            img = img[:12]\n",
    "        if img.shape[0] < 12:\n",
    "            raise ValueError(\"Image has less than 12 bands. TerraMind S2L2A requires 12.\")\n",
    "\n",
    "        img = img.astype(np.float32)\n",
    "        if img.max() > 2.0:\n",
    "            img = img / 10000.0\n",
    "        img = np.clip(img, 0, 1)\n",
    "\n",
    "        img_tensor = torch.from_numpy(img).unsqueeze(0)\n",
    "        img_tensor = F.interpolate(\n",
    "            img_tensor,\n",
    "            size=(self.image_size, self.image_size),\n",
    "            mode=\"bilinear\",\n",
    "            align_corners=False\n",
    "        ).squeeze(0)\n",
    "\n",
    "        data = {\"S2L2A\": img_tensor}\n",
    "\n",
    "        # ---------------- MASK ----------------\n",
    "        mask_name = sample_id.replace(\"img_\", \"mask_\")\n",
    "        mask_path = self.masks_path / f\"{mask_name}.tif\"\n",
    "        if not mask_path.exists():\n",
    "            mask_path = self.masks_path / f\"{mask_name}.tiff\"\n",
    "\n",
    "        with rasterio.open(str(mask_path)) as src:\n",
    "            mask = src.read(1)\n",
    "\n",
    "        mask_t = torch.from_numpy(mask).unsqueeze(0).unsqueeze(0).float()\n",
    "        mask_t = F.interpolate(mask_t, size=(self.image_size, self.image_size), mode=\"nearest\")\n",
    "        label = mask_t.squeeze(0).squeeze(0).long()\n",
    "\n",
    "        return data, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e2ab53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Found 228 samples in D:\\TerraMind_Dataset\\train\n",
      "âœ“ Found 57 samples in D:\\TerraMind_Dataset\\val\n",
      "\n",
      "âœ… DataLoaders created!\n",
      "  Train samples: 228\n",
      "  Val samples:   57\n",
      "  Train batches: 228\n",
      "  Val batches:   57\n",
      "\n",
      "ðŸ” Testing data loading...\n",
      "  Data keys: ['S2L2A']\n",
      "    S2L2A: torch.Size([1, 12, 224, 224]), range=[0.001, 0.403]\n",
      "  Labels: torch.Size([1, 224, 224]), unique=1058\n",
      "âœ… Data loading successful!\n"
     ]
    }
   ],
   "source": [
    "TRAIN_DATA_PATH = r\"D:\\TerraMind_Dataset\\train\"\n",
    "VAL_DATA_PATH   = r\"D:\\TerraMind_Dataset\\val\"\n",
    "\n",
    "try:\n",
    "    train_dataset = TerraMindDataset(\n",
    "        TRAIN_DATA_PATH,\n",
    "        image_size=CONFIG['image_size']\n",
    "    )\n",
    "\n",
    "    val_dataset = TerraMindDataset(\n",
    "        VAL_DATA_PATH,\n",
    "        image_size=CONFIG['image_size']\n",
    "    )\n",
    "\n",
    "    # Windows + rasterio MUST use num_workers=0\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=CONFIG['batch_size'],\n",
    "        shuffle=True,\n",
    "        num_workers=0,\n",
    "        pin_memory=False,\n",
    "        drop_last=True\n",
    "    )\n",
    "\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=CONFIG['batch_size'],\n",
    "        shuffle=False,\n",
    "        num_workers=0,\n",
    "        pin_memory=False\n",
    "    )\n",
    "\n",
    "    print(\"\\nâœ… DataLoaders created!\")\n",
    "    print(f\"  Train samples: {len(train_dataset)}\")\n",
    "    print(f\"  Val samples:   {len(val_dataset)}\")\n",
    "    print(f\"  Train batches: {len(train_loader)}\")\n",
    "    print(f\"  Val batches:   {len(val_loader)}\")\n",
    "\n",
    "    print(\"\\nðŸ” Testing data loading...\")\n",
    "    test_batch = next(iter(train_loader))\n",
    "    data, labels = test_batch\n",
    "\n",
    "    print(f\"  Data keys: {list(data.keys())}\")\n",
    "    for key, val in data.items():\n",
    "        print(f\"    {key}: {val.shape}, range=[{val.min():.3f}, {val.max():.3f}]\")\n",
    "\n",
    "    print(f\"  Labels: {labels.shape}, unique={len(torch.unique(labels))}\")\n",
    "    print(\"âœ… Data loading successful!\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c2c3d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from terratorch import BACKBONE_REGISTRY\n",
    "import pytorch_lightning as pl\n",
    "import torch.nn as nn\n",
    "\n",
    "class TerraMindFineTuner(pl.LightningModule):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.backbone = BACKBONE_REGISTRY.build(\n",
    "            config[\"model_name\"],\n",
    "            pretrained=True,\n",
    "            modalities=config[\"modalities\"],\n",
    "            bands=config[\"bands\"],\n",
    "            merge_method=\"mean\"\n",
    "        )\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Conv2d(768, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, config[\"num_classes\"], kernel_size=1)\n",
    "        )\n",
    "\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, data):\n",
    "        out = self.backbone(data)\n",
    "\n",
    "        if isinstance(out, list):\n",
    "            tokens = out[0]\n",
    "        else:\n",
    "            tokens = out\n",
    "\n",
    "        B, N, C = tokens.shape\n",
    "        H = W = int(N ** 0.5)\n",
    "\n",
    "        feat = tokens.transpose(1, 2).reshape(B, C, H, W)\n",
    "        feat = F.interpolate(feat, size=(self.config[\"image_size\"], self.config[\"image_size\"]), mode=\"bilinear\")\n",
    "\n",
    "        return self.decoder(feat)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        data, y = batch\n",
    "        y_hat = self.forward(data)\n",
    "\n",
    "        if y.max() >= self.config[\"num_classes\"]:\n",
    "            raise ValueError(f\"Label {y.max().item()} out of range num_classes={self.config['num_classes']}\")\n",
    "\n",
    "        loss = self.loss_fn(y_hat, y)\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        data, y = batch\n",
    "        y_hat = self.forward(data)\n",
    "        loss = self.loss_fn(y_hat, y)\n",
    "        self.log(\"val_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.AdamW(self.parameters(), lr=self.config[\"learning_rate\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35d2f71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Outputs will be saved to: c:\\Users\\sathv\\OneDrive\\Desktop\\Sathvik education\\5th Semester\\PE\\terramind_training_output\n"
     ]
    }
   ],
   "source": [
    "output_dir = Path('./terramind_training_output')\n",
    "output_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    dirpath=output_dir / 'checkpoints',\n",
    "    filename='terramind-{epoch:02d}-{val_loss:.4f}',\n",
    "    save_top_k=3,\n",
    "    monitor='val_loss',\n",
    "    mode='min',\n",
    "    save_last=True,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "early_stop_callback = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=8,\n",
    "    mode='min',\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "logger = TensorBoardLogger(\n",
    "    save_dir=output_dir / 'logs',\n",
    "    name='terramind_finetuning'\n",
    ")\n",
    "\n",
    "print(f\"âœ“ Outputs will be saved to: {output_dir.absolute()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a75b7fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ”„ Initializing model...\n",
      "\n",
      "ðŸ“Š Model Statistics:\n",
      "  Total params: 90,138,240\n",
      "  Trainable params: 90,138,240\n",
      "  Size: ~0.36 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: GPU available: False, used: False\n",
      "INFO:lightning.pytorch.utilities.rank_zero:GPU available: False, used: False\n",
      "INFO: TPU available: False, using: 0 TPU cores\n",
      "INFO:lightning.pytorch.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ“ Trainer initialized\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nðŸ”„ Initializing model...\")\n",
    "model = TerraMindFineTuner(CONFIG)\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"\\nðŸ“Š Model Statistics:\")\n",
    "print(f\"  Total params: {total_params:,}\")\n",
    "print(f\"  Trainable params: {trainable_params:,}\")\n",
    "print(f\"  Size: ~{total_params * 4 / 1e9:.2f} GB\")\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=CONFIG['max_epochs'],\n",
    "    accelerator='gpu' if torch.cuda.is_available() else 'cpu',\n",
    "    devices=1,\n",
    "    precision=CONFIG['precision'],\n",
    "    callbacks=[checkpoint_callback, early_stop_callback],\n",
    "    logger=logger,\n",
    "    log_every_n_steps=10,\n",
    "    gradient_clip_val=1.0,\n",
    "    accumulate_grad_batches=2,\n",
    "    enable_progress_bar=True,\n",
    "    enable_model_summary=True\n",
    ")\n",
    "\n",
    "print(\"\\nâœ“ Trainer initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f9ed83c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Œ TRUE GLOBAL LABEL RANGE:\n",
      "Min: 978\n",
      "Max: 7698\n",
      "â†’ Required num_classes = 7699\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import rasterio\n",
    "from pathlib import Path\n",
    "\n",
    "MASK_DIR = Path(r\"D:\\TerraMind_Dataset\\train\\masks\")\n",
    "\n",
    "min_label = 999999\n",
    "max_label = -1\n",
    "\n",
    "for m in MASK_DIR.glob(\"*.tif\"):\n",
    "    with rasterio.open(m) as src:\n",
    "        mask = src.read(1)\n",
    "        min_label = min(min_label, mask.min())\n",
    "        max_label = max(max_label, mask.max())\n",
    "\n",
    "print(\"ðŸ“Œ TRUE GLOBAL LABEL RANGE:\")\n",
    "print(\"Min:\", min_label)\n",
    "print(\"Max:\", max_label)\n",
    "print(\"â†’ Required num_classes =\", max_label + 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb3938d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "ðŸš€ STARTING TRAINING\n",
      "============================================================\n",
      "Estimated time: ~19 minutes\n",
      "============================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name     | Type             | Params | Mode \n",
      "------------------------------------------------------\n",
      "0 | backbone | TerraMindViT     | 87.3 M | train\n",
      "1 | decoder  | Sequential       | 2.8 M  | train\n",
      "2 | loss_fn  | CrossEntropyLoss | 0      | train\n",
      "------------------------------------------------------\n",
      "90.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "90.1 M    Total params\n",
      "360.553   Total estimated model params size (MB)\n",
      "182       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f88559ff3f504c57b5ded445192c5f45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âŒ Training error: Target 6198 is out of bounds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\sathv\\AppData\\Local\\Temp\\ipykernel_7516\\1195684400.py\", line 8, in <module>\n",
      "    trainer.fit(\n",
      "  File \"c:\\Users\\sathv\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py\", line 560, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"c:\\Users\\sathv\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pytorch_lightning\\trainer\\call.py\", line 49, in _call_and_handle_interrupt\n",
      "    return trainer_fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sathv\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py\", line 598, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"c:\\Users\\sathv\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py\", line 1011, in _run\n",
      "    results = self._run_stage()\n",
      "              ^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sathv\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py\", line 1053, in _run_stage\n",
      "    self._run_sanity_check()\n",
      "  File \"c:\\Users\\sathv\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py\", line 1082, in _run_sanity_check\n",
      "    val_loop.run()\n",
      "  File \"c:\\Users\\sathv\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pytorch_lightning\\loops\\utilities.py\", line 179, in _decorator\n",
      "    return loop_run(self, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sathv\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pytorch_lightning\\loops\\evaluation_loop.py\", line 145, in run\n",
      "    self._evaluation_step(batch, batch_idx, dataloader_idx, dataloader_iter)\n",
      "  File \"c:\\Users\\sathv\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pytorch_lightning\\loops\\evaluation_loop.py\", line 437, in _evaluation_step\n",
      "    output = call._call_strategy_hook(trainer, hook_name, *step_args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sathv\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pytorch_lightning\\trainer\\call.py\", line 329, in _call_strategy_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sathv\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pytorch_lightning\\strategies\\strategy.py\", line 412, in validation_step\n",
      "    return self.lightning_module.validation_step(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\sathv\\AppData\\Local\\Temp\\ipykernel_7516\\4233891047.py\", line 58, in validation_step\n",
      "    loss = self.loss_fn(y_hat, y)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sathv\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1775, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sathv\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1786, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sathv\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\loss.py\", line 1385, in forward\n",
      "    return F.cross_entropy(\n",
      "           ^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sathv\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\functional.py\", line 3458, in cross_entropy\n",
      "    return torch._C._nn.cross_entropy_loss(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "IndexError: Target 6198 is out of bounds.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ðŸš€ STARTING TRAINING\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Estimated time: ~{len(train_loader) * CONFIG['max_epochs'] // 60} minutes\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "try:\n",
    "    trainer.fit(\n",
    "        model,\n",
    "        train_dataloaders=train_loader,\n",
    "        val_dataloaders=val_loader\n",
    "    )\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"âœ… TRAINING COMPLETED!\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"\\nBest checkpoint: {checkpoint_callback.best_model_path}\")\n",
    "    print(f\"Best val loss: {checkpoint_callback.best_model_score:.4f}\")\n",
    "    \n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\nâš ï¸ Training interrupted\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nâŒ Training error: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd0b8362",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label min: 557\n",
      "Label max: 4804\n",
      "Label unique sample: tensor([557, 566, 568, 573, 575, 577, 580, 582, 584, 585, 586, 587, 588, 589,\n",
      "        590, 594, 595, 597, 599, 600, 601, 603, 606, 607, 608, 611, 616, 617,\n",
      "        619, 620, 622, 623, 625, 628, 629, 630, 632, 633, 634, 637, 638, 639,\n",
      "        640, 641, 642, 643, 644, 646, 648, 649])\n",
      "CONFIG num_classes: 10\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(train_loader))\n",
    "data, labels = batch\n",
    "\n",
    "print(\"Label min:\", labels.min().item())\n",
    "print(\"Label max:\", labels.max().item())\n",
    "print(\"Label unique sample:\", torch.unique(labels)[:50])\n",
    "print(\"CONFIG num_classes:\", CONFIG[\"num_classes\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f2edcc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TRAIN:\n",
      "Min: 978\n",
      "Max: 7698\n",
      "Unique: 5859\n",
      "\n",
      "VAL:\n",
      "Min: 1155\n",
      "Max: 7460\n",
      "Unique: 4622\n"
     ]
    }
   ],
   "source": [
    "# def scan_loader(loader, name):\n",
    "#     all_labels = []\n",
    "#     for _, y in loader:\n",
    "#         all_labels.append(y)\n",
    "#     all_labels = torch.cat(all_labels)\n",
    "\n",
    "#     print(f\"\\n{name}:\")\n",
    "#     print(\"Min:\", all_labels.min().item())\n",
    "#     print(\"Max:\", all_labels.max().item())\n",
    "#     print(\"Unique:\", len(all_labels.unique()))\n",
    "\n",
    "\n",
    "# scan_loader(train_loader, \"TRAIN\")\n",
    "# scan_loader(val_loader, \"VAL\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b17c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nðŸ”„ Loading best model...\")\n",
    "best_model = TerraMindFineTuner.load_from_checkpoint(\n",
    "    checkpoint_callback.best_model_path,\n",
    "    config=CONFIG\n",
    ")\n",
    "best_model.eval()\n",
    "if torch.cuda.is_available():\n",
    "    best_model = best_model.cuda()\n",
    "\n",
    "print(\"âœ“ Best model loaded!\")\n",
    "\n",
    "# Evaluate\n",
    "print(\"\\nðŸ“Š Evaluating...\")\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(val_loader, desc=\"Evaluating\"):\n",
    "        data, labels = batch\n",
    "        if torch.cuda.is_available():\n",
    "            data = {k: v.cuda() for k, v in data.items()}\n",
    "            labels = labels.cuda()\n",
    "        \n",
    "        outputs = best_model(data)\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        \n",
    "        all_preds.append(preds.cpu())\n",
    "        all_labels.append(labels.cpu())\n",
    "\n",
    "all_preds = torch.cat(all_preds)\n",
    "all_labels = torch.cat(all_labels)\n",
    "\n",
    "accuracy = (all_preds == all_labels).float().mean()\n",
    "print(f\"\\nâœ… Validation Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c637f741",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_predictions(model, dataloader, num_samples=4):\n",
    "    model.eval()\n",
    "    fig, axes = plt.subplots(num_samples, 3, figsize=(15, num_samples*4))\n",
    "    \n",
    "    if num_samples == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, (data, labels) in enumerate(dataloader):\n",
    "            if i >= num_samples:\n",
    "                break\n",
    "            \n",
    "            if torch.cuda.is_available():\n",
    "                data = {k: v.cuda() for k, v in data.items()}\n",
    "                labels = labels.cuda()\n",
    "            \n",
    "            outputs = model(data)\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            \n",
    "            first_modality = list(data.keys())[0]\n",
    "            img = data[first_modality][0].cpu().numpy()\n",
    "            \n",
    "            # RGB visualization\n",
    "            if img.shape[0] >= 3:\n",
    "                rgb = img[:3].transpose(1, 2, 0)\n",
    "                rgb = np.clip((rgb - rgb.min()) / (rgb.max() - rgb.min() + 1e-8), 0, 1)\n",
    "            else:\n",
    "                rgb = img[0]\n",
    "                rgb = (rgb - rgb.min()) / (rgb.max() - rgb.min() + 1e-8)\n",
    "            \n",
    "            axes[i, 0].imshow(rgb)\n",
    "            axes[i, 0].set_title(f'Input ({first_modality})')\n",
    "            axes[i, 0].axis('off')\n",
    "            \n",
    "            axes[i, 1].imshow(labels[0].cpu().numpy(), cmap='tab10')\n",
    "            axes[i, 1].set_title('Ground Truth')\n",
    "            axes[i, 1].axis('off')\n",
    "            \n",
    "            axes[i, 2].imshow(preds[0].cpu().numpy(), cmap='tab10')\n",
    "            axes[i, 2].set_title('Prediction')\n",
    "            axes[i, 2].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_dir / 'predictions.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(f\"âœ“ Saved to: {output_dir / 'predictions.png'}\")\n",
    "\n",
    "print(\"\\nðŸŽ¨ Creating visualizations...\")\n",
    "visualize_predictions(best_model, val_loader, num_samples=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f829a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_dir = output_dir / 'final_model'\n",
    "final_dir.mkdir(exist_ok=True)\n",
    "\n",
    "torch.save(best_model.state_dict(), final_dir / 'weights.pth')\n",
    "\n",
    "with open(final_dir / 'config.json', 'w') as f:\n",
    "    json.dump(CONFIG, f, indent=2)\n",
    "\n",
    "print(f\"\\nâœ“ Final model saved to: {final_dir.absolute()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13935c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ðŸ“Š TRAINING COMPLETE!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nConfiguration:\")\n",
    "print(f\"  Model: {CONFIG['model_name']}\")\n",
    "print(f\"  Modalities: {CONFIG['modalities']}\")\n",
    "print(f\"  Bands: {CONFIG['bands']}\")\n",
    "print(f\"  Dataset: {len(train_dataset)} train, {len(val_dataset)} val\")\n",
    "print(f\"  Epochs: {trainer.current_epoch + 1}\")\n",
    "print(f\"\\nResults:\")\n",
    "print(f\"  Best val loss: {checkpoint_callback.best_model_score:.4f}\")\n",
    "print(f\"  Final accuracy: {accuracy*100:.2f}%\")\n",
    "print(f\"\\nOutputs: {output_dir.absolute()}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nðŸ“ˆ View training logs:\")\n",
    "print(f\"%load_ext tensorboard\")\n",
    "print(f\"%tensorboard --logdir {output_dir / 'logs'}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
